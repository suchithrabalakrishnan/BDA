{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_classes(xs, ys):\n",
    "\n",
    "    freqs = Counter(ys)\n",
    "\n",
    "    # the least common class is the maximum number we want for all classes\n",
    "    max_allowable = freqs.most_common()[-1][1]\n",
    "    num_added = {clss: 0 for clss in freqs.keys()}\n",
    "    new_ys = []\n",
    "    new_xs = []\n",
    "    for i, y in enumerate(ys):\n",
    "        if (num_added[y] < max_allowable):\n",
    "            new_ys.append(y)\n",
    "            new_xs.append(xs[i])\n",
    "            num_added[y] += 1\n",
    "    return new_xs, new_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review,\"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkEnglishDict(clean_review,english_vocab):\n",
    "    #english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    meaningful_words = [w for w in clean_review.split() if w in english_vocab]  \n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text.split() if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data from disk and split into lines\n",
    "# we use .strip() to remove the final (empty) line\n",
    "with open(\"xaa.json\",encoding=\"utf8\") as f:\n",
    "    reviews = f.read().strip().split(\"\\n\")\n",
    "reviews = [json.loads(review) for review in reviews] \n",
    "texts = [review['text'] for review in reviews]\n",
    "stars = [review['stars'] for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the texts set ...\n",
      "\n",
      "Review 5000 of 20000\n",
      "\n",
      "Review 10000 of 20000\n",
      "\n",
      "Review 15000 of 20000\n",
      "\n",
      "Review 20000 of 20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords, special characters\n",
    "print (\"Cleaning and parsing the texts set ...\\n\")\n",
    "clean_reviews = []\n",
    "num_reviews=len(texts)\n",
    "for i in range( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ))                                                                    \n",
    "    clean_reviews.append( review_to_words( texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 20000\n",
      "\n",
      "Review 10000 of 20000\n",
      "\n",
      "Review 15000 of 20000\n",
      "\n",
      "Review 20000 of 20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing for english words\n",
    "meaningful_clean_reviews=[]\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ))                                                                    \n",
    "    meaningful_clean_reviews.append(checkEnglishDict(clean_reviews[i],english_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location is everything and this hotel has it! The reception is inviting and open 24 hours. They are very helpful and have a lot of patience answering all my questions about where to go etc. there is also a lounge open 24 hours with snack-type food. Breakfast is continental-style so if you want heartier fare look elsewhere though you don't have to go far. The bus and train stations are right across the street so it's easy access to the airport or anywhere else you may want to go. Turn uphill to old town or cross the bridge to new town. The room with a view i got was spacious and comfortable though it's a bit of a maze to find it-just follow the signs. The windows are double paned so the room is quiet plus i was on the 5th floor which helps. It's a bit pricey but still one of the best values i found!\n",
      "\n",
      "***\n",
      "location everything hotel reception inviting open hours helpful lot patience answering questions go etc also lounge open hours snack type food breakfast continental style want heartier fare look elsewhere though go far bus train stations right across street easy access airport anywhere else may want go turn uphill old town cross bridge new town room view got spacious comfortable though bit maze find follow signs windows double paned room quiet plus th floor helps bit pricey still one best values found\n",
      "\n",
      "***\n",
      "location everything hotel reception inviting open helpful lot patience go also lounge open snack type food breakfast continental style want fare look elsewhere though go far bus train right across street easy access airport anywhere else may want go turn uphill old town cross bridge new town room view got spacious comfortable though bit maze find follow double paned room quiet plus th floor bit still one best found\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "print(texts[i])\n",
    "print(\"\\n***\")\n",
    "print(clean_reviews[i])\n",
    "print(\"\\n***\")\n",
    "print(meaningful_clean_reviews[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Created\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 20000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "data_features = vectorizer.fit_transform(meaningful_clean_reviews)\n",
    "\n",
    "data_features = data_features.toarray()\n",
    "print(\"Created\")\n",
    "\n",
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "threshold_count=2\n",
    "low_freq_words=[]\n",
    "#for tag, count in zip(vocab, dist):\n",
    "low_freq_words=[tag  for tag, count in zip(vocab, dist) if(count<threshold_count)]\n",
    "    #print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 20001\n",
      "\n",
      "Review 10000 of 20001\n",
      "\n",
      "Review 15000 of 20001\n",
      "\n",
      "Review 20000 of 20001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#removes low frequency words from the reviews\n",
    "num_reviews=len(meaningful_clean_reviews)\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ))                                                                    \n",
    "    words = meaningful_clean_reviews[i].split()\n",
    "    new_words = [w for w in words if not w in low_freq_words]\n",
    "    meaningful_clean_reviews[i]=\" \".join( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-223-2641a131b79d>, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-223-2641a131b79d>\"\u001b[1;36m, line \u001b[1;32m63\u001b[0m\n\u001b[1;33m    text_file = open(“SlangSD.text”, “r”)\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "print(Counter(stars))\n",
    "balanced_x, balanced_y = balance_classes(texts, stars)\n",
    "print(Counter(balanced_y))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "# This vectorizer breaks text into single words and bi-grams\n",
    "# and then calculates the TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    " \n",
    "# the 'fit' builds up the vocabulary from all the reviews\n",
    "# while the 'transform' step turns each indivdual text into\n",
    "# a matrix of numbers.\n",
    "vectors = vectorizer.fit_transform(balanced_x)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, balanced_y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    " \n",
    "# initialise the SVM classifier\n",
    "classifier = LinearSVC()\n",
    " \n",
    "# train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "preds = classifier.predict(X_test)\n",
    "print(list(preds[:10]))\n",
    "print(y_test[:10])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "#import pip\n",
    "#pip.main(['install','pyenchant'])\n",
    "\n",
    "import enchant\n",
    "\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "from enchant.tokenize import get_tokenizer, EmailFilter\n",
    "\n",
    "tknzr = get_tokenizer(\"en_US\")\n",
    "[w for w in tknzr(\"send an email to fake@example.com please\")] #[('send', 0), ('an', 5), ('email', 8), ('to', 14), ('fake@example.com', 17), ('please', 34)]\n",
    "tknzr = get_tokenizer(\"en_US\",[EmailFilter])\n",
    "[w for w in tknzr(\"send an email to fake@example.com please\")]\n",
    "#[('send', 0), ('an', 5), ('email', 8), ('to', 14), ('please', 34)]\n",
    "\n",
    "tknzr\n",
    "\n",
    "import enchant\n",
    "import enchant.checker\n",
    "from enchant.checker.CmdLineChecker import CmdLineChecker\n",
    "chkr = enchant.checker.SpellChecker(\"en_US\")\n",
    "chkr.set_text(\"this is sme example txt\")\n",
    "cmdln = CmdLineChecker()\n",
    "cmdln.set_checker(chkr)\n",
    "cmdln.run()\n",
    "\n",
    "text_file = open(“SlangSD.text”, “r”) \n",
    "#print file.read() \n",
    "\n",
    "text_file= open(\"SlangSD.txt\",\"r\", encoding= \"utf8\")\n",
    "lines = text_file.read()#.split(',')\n",
    "\n",
    "type(lines)\n",
    "\n",
    "data = pd.read_csv('SlangSD.txt', sep=\"\\t\", header=None,error_bad_lines=False, encoding= \"utf8\")\n",
    "#data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "\n",
    "data.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pip\n",
    "#pip.main(['install','stemming'])\n",
    "\n",
    "\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "#documents = [[stem(word) for word in sentence.split(\" \")] for sentence in documents]\n",
    "\n",
    "#def stemming(meaningful_clean_review):\n",
    " #   ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(meaningful_clean_reviews[3])\n",
    "aa=[]\n",
    "aa=[stem(w) for w in meaningful_clean_reviews[3].split()]\n",
    "#    print(ps.stem(w))\n",
    "bb=\" \".join(aa)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
